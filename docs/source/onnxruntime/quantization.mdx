<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Quantization

ðŸ¤— Optimum provides an `optimum.onnxruntime` package that enables you to apply quantization on many model hosted on the ðŸ¤— hub using the [ONNX Runtime](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/README.md) quantization tool.

## Create `ORTQuantizer` class

The `ORTQuantizer` class is used to quantize your onnx model. The class can be initialized using the `from_pretrained` method, which supports different checkpoint formats.

1. Using a vanilla transformers (non converted) PyTorch checkpoint

```python
from optimum.onnxruntime import ORTQuantizer

# create a quantizer from a vanilla PyTorch checkpoint by converting the model
quantizer = ORTQuantizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english", from_transformers=True, task="text-classification")

quantizer.fit(...)
```

2. Using a already converted onnx model from the [Hugging Face Hub](https://huggingface.co/optimum/distilbert-base-uncased-finetuned-sst-2-english)

```python
from optimum.onnxruntime import ORTQuantizer

# create a quantizer from a converted onnx model from the hugging face hub
quantizer = ORTQuantizer.from_pretrained("optimum/distilbert-base-uncased-finetuned-sst-2-english", task="text-classification")

quantizer.fit(...)
```

3. Using a already converted local onnx model. 

```python
from optimum.onnxruntime import ORTQuantizer

# This assumes a model.onnx in the path/to/model
quantizer = ORTQuantizer.from_pretrained("path/to/model")

quantizer.fit(...)
```

4. Using a already initialized `ORTModelForXXX` class.

```python
from optimum.onnxruntime import ORTQuantizer

# This assumes a that ort_model is a `ORTModelForXXX` class, e.g. ORTModelForSequenceClassification
quantizer = ORTQuantizer.from_pretrained(ort_model)

quantizer.fit(...)
```


## ORTQuantizer

[[autodoc]] onnxruntime.quantization.ORTQuantizer
    - all