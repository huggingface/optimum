<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Optimum pipelines for inference

The [`optimum_pipeline`] makes it simple to use models from the [Model Hub](https://huggingface.co/models) for accelerated inference on a variety of tasks such as text classification.
Even if you don't have experience with a specific modality or understand the code powering the models, you can still use them with the [`optimum_pipeline`]! This tutorial will teach you to:

<Tip>

You can also use the `transformers.pipeline` and provide your `OptimumModel`.

</Tip>

Currenlty supported tasks are:

**Onnxruntime**

* `feature-extraction`
* `text-classification`
* `token-classification`
* `question-answering`
* `zero-shot-classification`
* `text-generation`

## Optimum pipeline usage

While each task has an associated [`optimum_pipeline`], it is simpler to use the general [`pipeline`] abstraction which contains all the specific task pipelines. 
The [`optimum_pipeline`] automatically loads a default model and tokenizer capable of inference for your task. 

1. Start by creating a [`optimum_pipeline`] and specify an inference task:

```py
>>> from optimum import optimum_pipeline

>>> classifier = optimum_pipeline(task="text-classification", accelerator="ort")

```

2. Pass your input text to the [`optimum_pipeline`]:

```python
>>> classifier("I like you. I love you.")
```

### Use a vanilla transformers model and convert

The [`optimum_pipeline`] accepts supported model from the [Model Hub](https://huggingface.co/models). 
There are tags on the Model Hub that allow you to filter for a model you'd like to use for your task. 
Once you've picked an appropriate model, load it with the `from_pretrained("{model_id}",from_transformers=True)` method for corresponding `ORTModelFor*` 
and [`AutoTokenizer'] class. For example, load the [`ORTModelForQuestionAnswering`] class for a question answering modeling task:

```py
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering
>>> from optimum import optimum_pipeline

>>> tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")
>>> model = ORTModelForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2",from_transformers=True)

>>> onnx_qa = optimum_pipeline("question-answering", model=model, tokenizer=tokenizer)
>>> question = "Whats my name?"
>>> context = "My Name is Philipp and I live in Nuremberg."

>>> pred = onnx_qa(question=question, context=context)
```

### Use optimum model

The [`optimum_pipeline`] is tightly integrated with [Model Hub](https://huggingface.co/models) and can load optimized models directly, e.g. Onnxruntime. 
There are tags on the Model Hub that allow you to filter for a model you'd like to use for your task. 
Once you've picked an appropriate model, load it with the `.from_pretrained` method for corresponding `ORTModelFor*` 
and [`AutoTokenizer'] class. For example, load the [`ORTModelForQuestionAnswering`] class for a question answering modeling task:

```py
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering
>>> from optimum import optimum_pipeline

>>> tokenizer = AutoTokenizer.from_pretrained("optimum/roberta-base-squad2")
>>> model = ORTModelForQuestionAnswering.from_pretrained("optimum/roberta-base-squad2")

>>> onnx_qa = optimum_pipeline("question-answering", model=model, tokenizer=tokenizer)
>>> question = "Whats my name?"
>>> context = "My Name is Philipp and I live in Nuremberg."

>>> pred = onnx_qa(question=question, context=context)
```


### Optimizing and Quantizing in Pipelines

_TODO: Rework this is not valid anymore_

The [`optimum_pipeline`] can not only run inference on vanilla Onnxruntime checkpoints you can also use checkpoints optimized with `ORTQuantizer` and `ORTOptimizer`
Below you can find two examples on how you could [~`ORTOptimizer`] and [~`ORTQuantizer`] to optimize/quantize your model and use it for inference afterwards.

### Quantizing with [~`ORTQuantizer`]

```python
>>> from pathlib import Path
>>> from optimum.onnxruntime import ORTModelForSequenceClassification, ORTQuantizer
>>> from optimum.onnxruntime.configuration import AutoQuantizationConfig
>>> from optimum.pipelines import optimum_pipeline
>>> from transformers import AutoTokenizer

# define model_id and load tokenizer
>>> model_id = "distilbert-base-uncased-finetuned-sst-2-english"
>>> tokenizer = AutoTokenizer.from_pretrained(model_id)
>>> save_path = Path("optimum_model")
>>> save_path.mkdir(exist_ok=True)

# use ORTQuantizer to export the model and define quantization configuration
>>> quantizer = ORTQuantizer.from_pretrained(model_id, feature="sequence-classification")
>>> qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)

# apply the quantization configuration to the model
>>> quantizer.export(
    onnx_model_path=save_path / "model.onnx",
    onnx_quantized_model_output_path=save_path / "model-quantized.onnx",
    quantization_config=qconfig,
    )
>>> quantizer.model.config.save_pretrained(save_path) # saves config.json 

# load optimized model from local path or repository
>>> model = ORTModelForSequenceClassification.from_pretrained(save_path,file_name="model-quantized.onnx")

# create transformers pipeline
>>> onnx_clx = optimum_pipeline("text-classification", model=model, tokenizer=tokenizer)
>>> text = "I like the new ORT pipeline"
>>> pred = onnx_clx(text)
>>> print(pred)

# save model & push model to the hub
>>> tokenizer.save_pretrained("new_path_for_directory")
>>> model.save_pretrained("new_path_for_directory")
>>> model.push_to_hub("new_path_for_directory",
                  repository_id="my-onnx-repo",
                  use_auth_token=True
                  )
```

### Optimizing with [~`ORTOptimizer`]

```python
>>> from pathlib import Path
>>> from optimum.onnxruntime import ORTModelForSequenceClassification, ORTOptimizer
>>> from optimum.onnxruntime.configuration import OptimizationConfig
>>> from optimum.pipelines import optimum_pipeline

# define model_id and load tokenizer
>>> model_id = "distilbert-base-uncased-finetuned-sst-2-english"
>>> tokenizer = AutoTokenizer.from_pretrained(model_id)
>>> save_path = Path("optimum_model")
>>> save_path.mkdir(exist_ok=True)

# use ORTOptimizer to export the model and define quantization configuration
>>> optimizer = ORTOptimizer.from_pretrained(model_id, feature="sequence-classification")
>>> optimization_config = OptimizationConfig(optimization_level=2)

# apply the optimization configuration to the model
>>> optimizer.export(
    onnx_model_path=save_path / "model.onnx",
    onnx_optimized_model_output_path=save_path / "model-optimized.onnx",
    optimization_config=optimization_config,
)
>>> optimizer.model.config.save_pretrained(save_path) # saves config.json 

# load optimized model from local path or repository
>>> model = ORTModelForSequenceClassification.from_pretrained(save_path,file_name="model-optimized.onnx")

# create transformers pipeline
>>> onnx_clx = optimum_pipeline("text-classification", model=model, tokenizer=tokenizer)
>>> text = "I like the new ORT pipeline"
>>> pred = onnx_clx(text)
>>> print(pred)

# save model & push model to the hub
>>> tokenizer.save_pretrained("new_path_for_directory")
>>> model.save_pretrained("new_path_for_directory")
>>> model.push_to_hub("new_path_for_directory",
                  repository_id="my-onnx-repo",
                  use_auth_token=True)
```

## Transformers pipeline usage

The [`optimum_pipeline`] is just a light wrapper around the `transformers.pipeline` to enable checks for supported tasks and additional features
, like quantization and optimization. This being said you can use the `transformers.pipeline` and just replace your `AutoFor*` with the optimum 
 `ORTModelFor*` class. 

```diff
from transformers import AutoTokenizer, pipeline
-from transformers import AutoModelForQuestionAnswering
+from optimum.onnxruntime import ORTModelForQuestionAnswering

-model = AutoModelForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2")
+model = ORTModelForQuestionAnswering.from_transformers("optimum/roberta-base-squad2")
tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")

onnx_qa = pipeline("question-answering",model=model,tokenizer=tokenizer)

question = "Whats my name?"
context = "My Name is Philipp and I live in Nuremberg."
pred = onnx_qa(question, context)
```