<!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# ðŸ¤— Optimum notebooks

You can find here a list of the notebooks associated with each accelerator in ðŸ¤— Optimum.                                                                                                                    |

## Optimum Habana examples

| Notebook                                                                                                                                                                               | Description                                                                                                                                                                       |  Colab                                                                                                                                                                                                          |        Studio Lab                                                                                                                                                                                                   |
|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) |

## Optimum Intel examples

| Notebook                                                                                                                                                                               | Description                                                                                                                                                                       |                                     Colab                                                                                                                                                                                                          |        Studio Lab                                                                                                                                                                                                   |
|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| [How to quantize a model with Intel Neural Compressor for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb) | Show how to apply static, dynamic and aware training quantization on a model using Intel [Neural Compressor](https://github.com/intel/neural-compressor) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb) |
| [How to quantize a model with OpenVINO NNCF for question answering](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb) | Show how to apply post-training quantization on a question answering model using [NNCF](https://github.com/openvinotoolkit/nncf) and to accelerate inference with OpenVINO| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb) |


## Optimum ONNX Runtime examples

| Notebook                                                                                                                                                                    | Description                                                                                                                                    |                                                                        Colab                                                                                                                                                                                                          |        Studio Lab                                                                                                                                                                                                   |
|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| [How to quantize a model with ONNX Runtime for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb) | Show how to apply static and dynamic quantization on a model using [ONNX Runtime](https://github.com/microsoft/onnxruntime) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb) |
| [How to fine-tune a model for text classification with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)             | Show how to DistilBERT model on GLUE tasks using [ONNX Runtime](https://github.com/microsoft/onnxruntime).                                     | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)          | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb) |
| [How to fine-tune a model for summarization with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)                         | Show how to fine-tune a T5 model on the BBC news corpus.                                                                                       | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)                |                [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb) |


## Optimum Graphcore examples

| Notebook                                                                                                                                              | Description                                                                                                 |    Colab                                                                                                                                                                                                                                                                                                         |
|:------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Introduction to Optimum Graphcore](https://github.com/huggingface/optimum-graphcore/blob/main/notebooks/introduction_to_optimum_graphcore.ipynb)     | Introduce Optimum-Graphcore with a BERT fine-tuning example.                                                | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-graphcore/blob/main/notebooks/introduction_to_optimum_graphcore.ipynb) |
| [Train an external model](https://github.com/huggingface/optimum-graphcore/blob/main/notebooks/external_model.ipynb)                                  | Show how to train an external model that is not supported by Optimum or Transformers.                       |   [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-graphcore/blob/main/notebooks/external_model.ipynb)                                                                                                                                                                                                                                                                                                           |
| [Train your language model](https://github.com/huggingface/optimum-graphcore/blob/main/notebooks/language_modelling_from_scratch.ipynb)               | Show how to train a model for causal or masked language modelling from scratch.                             |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-graphcore/blob/main/notebooks/language_modelling_from_scratch.ipynb)                                                                                                                                                                                                                                                                                                            |
| [How to fine-tune a model on text classification](https://github.com/huggingface/optimum-graphcore/blob/main/notebooks/text_classification.ipynb)     | Show how to preprocess the data and fine-tune a pretrained model on any GLUE task.                          | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-graphcore/blob/main/notebooks/text_classification.ipynb)                |
| [How to fine-tune a model on language modeling](https://github.com/huggingface/optimum-graphcore/blob/main/notebooks/language_modeling.ipynb)         | Show how to preprocess the data and fine-tune a pretrained model on a causal or masked LM task.             |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-graphcore/blob/main/notebooks/language_modeling.ipynb)                |
| [How to fine-tune a model on token classification](https://github.com/huggingface/optimum-graphcore/blob/main/notebooks/token_classification.ipynb)   | Show how to preprocess the data and fine-tune a pretrained model on a token classification task (NER, PoS). |   [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-graphcore/blob/main/notebooks/token_classification.ipynb)                |                                                                                                                                                                                                                                                                                                          |
| [How to fine-tune a model on question answering](https://github.com/huggingface/optimum-graphcore/blob/main/notebooks/question_answering.ipynb)       | Show how to preprocess the data and fine-tune a pretrained model on SQUAD.                                  |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-graphcore/blob/main/notebooks/question_answering.ipynb)                |                                                                                                                                                                                                                                                                                                            |
| [How to fine-tune a model on multiple choice](https://github.com/huggingface/optimum-graphcore/blob/main/notebooks/multiple_choice.ipynb)             | Show how to preprocess the data and fine-tune a pretrained model on SWAG.                                   |   [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-graphcore/blob/main/notebooks/multiple_choice.ipynb)                |                                                                                                                                                                                                                                                                                                                                         |
| [How to fine-tune a model on translation](https://github.com/huggingface/optimum-graphcore/blob/main/notebooks/translation.ipynb)                     | Show how to preprocess the data and fine-tune a pretrained model on WMT.                                    |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-graphcore/blob/main/notebooks/translation.ipynb)                |                                                                                                                                                                                                                                                                                                              |
| [How to fine-tune a model on summarization](https://github.com/huggingface/optimum-graphcore/blob/main/notebooks/summarization.ipynb)                 | Show how to preprocess the data and fine-tune a pretrained model on XSUM.                                   |   [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-graphcore/blob/main/notebooks/summarization.ipynb)                |                                                                                                                                                                                                                                                                                                                |
| [How to fine-tune a model on audio classification](https://github.com/huggingface/optimum-graphcore/blob/main/notebooks/audio_classification.ipynb)   | Show how to preprocess the data and fine-tune a pretrained Speech model on Keyword Spotting                 |   [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-graphcore/blob/main/notebooks/audio_classification.ipynb)                |                                                                                                                                                                                                                                                                                                                                           |
| [How to fine-tune a model on image classfication](https://github.com/huggingface/optimum-graphcore/blob/main/notebooks/image_classification.ipynb)    | Show how to preprocess the data and fine-tune a pretrained model on image classification.                   |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-graphcore/blob/main/notebooks/image_classification.ipynb)                |                                                                                                                                                                                                                                                                                                                                              |
| [wav2vec 2.0 Inference on IPU](https://github.com/huggingface/optimum-graphcore/blob/main/notebooks/wav2vec2/wav2vec2-inference-checkpoint.ipynb)     | How to run inference on the wav2vec 2.0 model with PyTorch on the Graphcore IPU-POD16 system.               |   [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-graphcore/blob/main/notebooks/wav2vec2/wav2vec2-inference-checkpoint.ipynb)                |                                                                                                                                                                                      